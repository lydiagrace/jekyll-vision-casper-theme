---
layout: post
title:  "“Exposing” Racism in Photography Technology"
date:   2020-05-08 
author: Lydia Allen
description: "ENGL 477 Digital Humanities"

Photography technology companies have a history of looking at their products from one point of view and creating their “default” setting as fitting for white faces, as far back as 1959 with Kodak (Benjamin, 104).  Today, this history of centering white faces when developing technology around images has resulted in facial recognition technologies that can outright perpetuate racial biases, make mistakes when used by or for people with darker skin color, and ultimately affect people’s lives in major ways when utilized institutionally.  Looking back at the way photo technology has seen - or not seen - human beings gives us an insight to how the mistakes made in modern photo technologies can continue. As Benjamin says in her book section “Exposing Whiteness”, “You might be thinking, surely this is no longer an issue” (108).  For white people, the idea that photo settings have not worked for some people in the past might be completely new information, because they have always been served - their skin color has always been centered.  Ruha Benjamin elaborates on the idea that when someone or something is centered, someone or something else is being decentered.  White people, whose skin color has always been “centered” throughout history, must actively expose themselves to their own privilege, which includes understanding who and what has been decentered, even in ways that can seem more trivial– like the settings on a camera – because they have real consequences. 

Small Complaints to Apartheid
What might seem at first like a small complaint - “My camera exposure settings make it difficult to photograph my face” - is only one aspect of many related issues in the history of photo technology and race.  For a white person to write off even the small aspect of being able to take a good photograph is unfair when they fit the “default” settings for every technology that exists.  “Can the camera be racist?” an article in The Guardian asks.  What we know for a fact is that technology can do what it is told to do; in the case of early cameras, the technology was told to take photos of white skin, and white skin only. No one told the technology to take photos of black skin. But it wasn’t that they didn’t find a way when they needed to keep black people down, as shown by the use of Polaroid cameras with “boost” flash buttons to take photos of black people during the South Africa apartheid for use as passbooks, a way to track black people and maintain segregation (The Guardian).  In fact, the precise technology of this button made an artist hypothesize about its intentional racist purpose: “"Black skin absorbs 42% more light. The button boosts the flash exactly 42%," Broomberg explained. "It makes me believe it was designed for this purpose” (The Guardian).  Therefore Polariod, the company out of United Kingdom, was intricately involved in the South African apartheid until they pulled out around 1977 after protests.  Their images were essential to Pass laws, which were used to keep black people outside of the urban white areas and severely limited their movements (Benjamin, 106). 

Kodak Shirley
However, unless it was in order to surveil black folks, photo technology in the mid-20th century was hardwired to NOT work for dark skin color, because the light range was that narrow.  The default setting with the narrow light range was designed for a person with the skin color of Polaroid’s first model “Shirley,” who was a white woman in the 50s.  Her skin tone, Lorna Roth says, has “been the recognized skin ideal standard for most North American analogue photo labs since the early part of the twentieth century and they continue to function as the dominant norm” (Looking at Shirley).  Roth discusses the origin of the choice of Shirley, who was the industry standard in North American photo labs that were owned by largely men.  She brings up an interesting point about Shirley being a woman - a choice made by the male owners, no doubt.  The task Shirley was used for could have easily been done by a male model, and it might have even have addressed the contrast issue earlier: “Alone or in groups, men wearing coloured shirts with similar skin-tone ranges and hair colours to those of female Shirleys could have worked as effectively as a reference standard—especially if they had had beards or moustaches. These actually might have provided technicians with the practice to deal more effectively with the contrast issue.”  

Figure A. 

Had Shirley been a man with a little beard scruff, the exposure settings might have been less oversimplified for her smooth consistent “normal” face (See Figure A) - yet Roth found few uses of men for reference modeling.  When you think about it, how much sense does it make to base everything around “beautiful” “thin, able-bodied” “white” “eyebrowed” “Western European” women when we know full well that she simply isn’t the majority of the people you see on a daily basis? It is what we always do, that is the entire essence of our modeling industry… But say, as a photographer making money, you’d desire the ability to photograph a majority of people, not only this fairly homogenous group of “beautiful” specifically-white women.  Roth says that while all this technology was being produced, at the same time "emerged a masculinist collection of sexy female imagery to tinker with, pin up on lab walls, and use in the colour balancing process” which isn’t hard to visualize with all of her evidence.  Sometimes, we give history the benefit of the doubt - maybe it wasn’t intentional. But often there was a creepy room of powerful men laughing to each other, and Roth bringing this imagery up makes it seem less “unintentional.”  It isn’t rocket science - it wasn’t that contrast and exposure was just mystifying in the 50s - they knew what they were doing. One photographer as quoted by Benjamin said, “It turns out, film stock’s failures to capture dark skin aren’t a technical issue, they’re a choice” (104). 
An extremely saddening improvement came eventually: “It was only when Kodak's two biggest clients – the confectionary and furniture industries – complained that dark chocolate and dark furniture were losing out that it came up with a solution” (The Guardian).  In the 1980s, a new film stock was created to address the past decades’ issues with underexposure, but it wasn’t provided for humans - it was released for taking photos of “a dark horse in low light” which has unsettling implications.  Not only did Kodak decide to not correct their errors openly, they chose to reference their correction to animals and furniture instead of people.  Benjamin discusses how these things - animals, chocolate, furniture - were what made companies finally make changes, having ignored complaints by black parents that their children’s school picture day photos were poor quality (105).  Time and again, technology companies reminded black people that they were not a demographic they cared to serve. 

Racism in Modern Facial Recognition Software
	There have been innumerous mistakes in software that have perpetuated racist judgements, like when Google Photos tagged two black friends as gorillas, which as Benjamin says “goes back for centuries” (110). The man who tweeted that out said later, “What kind of sample image data you collected that would result in this son?” (Jacky Alcine).  When companies started rolling out software that unlocks your tech products with face recognition, black people reported the camera being unable to recognize them. One example of this was the 2009 viral video “HP computers are racist” which was inexplicable evidence that an HP webcam followed “white Wanda” around when she moved, but didn’t recognize “black Desi” when he got in frame, and the camera didn’t move around at all for his face.  He said the famous words “I think my blackness is interfering” with the camera’s ability to follow him, words that frame his own blackness as the issue when he knows it is not.  
In 2019, the ACLU tested Amazon’s facial recognition software called Rekognition (which may as well be called “Selektion”??), and during the test, the software made many major mistakes.  It falsely identified 28 US Congresspeople with people in mugshots. 40% of the incorrect matches were for people of color, even though they only make up 20% of Congress (Snow). It could not be more irresponsible to use machine learning to train a facial recognition system using mugshots. Mugshots are actually before someone is convicted, so sharing them publicly is also terribly problematic.  But training computers to understand that criminals look like the people American police arrest is terrifying.  It also clearly doesn’t work – and if your billion dollar company’s software doesn’t even kind of work in 2019, I don’t know what you are doing, but maybe you should just give up.  However, as Professor Kelly Gates says in “Can Computers Be Racist?,” it doesn’t matter if the software works: “it is important to understand that even if a perfectly accurate facial recognition system were possible—which it is not—it still could be used in ways that reproduce structural inequalities” (15). The use of facial recognition software by law enforcement is a mistake that, when – not if – the software is abused, it will have serious life consequences, and they will be placed unfairly on the black community to deal with. 
	Benjamin maintains throughout her book that it doesn’t have to be this way. Technology is “racist” because of choices that have been made – and that means different choices can be made.  As she says, “The technical capacity was always there, but social awareness and incentives to ensure fair representation online were lacking” (94). 


COVID-19 Connection
I have happened to see some 2020 COVID-19 Youtube videos by women titled “How to look good in your Zoom meetings” or something along those lines. Something that stuck out to me, since I was writing this paper, was the suggestion by white women (today’s Shirleys?) to wear a light or neutral colored shirt, because wearing a dark shirt will cause the video to highly overexpose your face and make your face shine like a light bulb.  I assume that few white viewers will think to themselves about how that advice would not work for people who do not have such white skin.  One Canadian Youtuber (See Figure B) made it clear that her suggestion was for “marshmallows” like herself, which was important to clarify, and not terribly difficult. The thing is, perhaps technology “shouldn’t” have anything to do with race - and that makes sense, certainly. But regardless, humans create and use technology, and that means knowing more about it than what it means for you specifically because you know you aren’t the only type of person. To only understand photo technology for your white face is not useful, if not only because it means you have an extremely limited understanding of how photography works. Of course, it also means you can’t take photos of people different than yourself.  
Figure B.

Zoom posted something interesting on their own blog about the subject of what light bulbs to use: “A light with a high color rendering index (CRI) is able to show the true colors of every object in a room. Low CRI lights may be advantageous for people with faces that have imperfections on the skin, since it makes them easier to mask. High CRI lights will work well for people with darker skin tones, since it helps reveal the nuances in pigment that would otherwise have been glossed over with a simpler incandescent bulb. The kind of light bulb you choose affects how much of your true beauty shines in more ways than you think, doesn’t it?” (Zoom Blog).  This was the one mention I could find by the company Zoom about lighting. It sounded a little unnatural – I would be shocked to find someone out there who would change their light bulbs for a video conference.  This makes sense from a filmography company, but not a video conference company, so it leaves me confused. 


---
You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

To add new posts, simply add a file in the `_posts` directory that follows the convention `YYYY-MM-DD-name-of-post.ext` and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:

{% highlight ruby %}
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
{% endhighlight %}

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: http://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/
